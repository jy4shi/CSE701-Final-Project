# Multivariate Polynomial Optimization in C++

## Summary

The purpose of this program is to find a stationary point (local minimum, local maximum, or saddle point) of any polynomial by implementing optimization algorithms including Gradient Descent and Newton’s Method. Before running the program, users must create a text file that lists the necessary parameters for optimization in the current workspace folder. While compiling, the program will validate the input data from the text file, perform the optimization algorithms, and then store the results in separate text files. For further information on how the optimization algorithms are performed, please check the documentation file `Doxyfile` generated by Doxygen.

## Author

This code was written by Jun Yi (Jason) Shi (shij82@mcmaster.ca) for the final project of the course CSE 701: Foundations of Modern Scientific Programming at McMaster University.

## Usage

### Step 1

Create a text file named `input_function.txt`, type out the necessary parameters (see Restrictions on `input_function.txt` below), and then save the text file in the current workspace folder

### Step 2

Compile and run `main.cpp`

### Step 3

If the optimization algorithms were performed successfully, 4 new text files would be created in the current workspace folder:

- `output_iterations_gradient_descent.txt`
- `output_iterations_newtons_method.txt`
- `output_results_gradient_descent.txt`
- `output_results_newtons_method.txt`

Open these files to see the results of the optimization algorithms

### Restrictions on input_function.txt

The text file `input_function.txt` stores the necessary parameters for optimization, including min/max, polynomial function, initial point, tolerance, and max iterations. When the program is reading this file, all spaces will be ignored. The text file `input_function.txt` should to have 5 lines in total:

#### First line: max/min

- Either "min" or "max"
- "min" will allow the program to perform gradient descent and find a local minimum
- "max" will allow the program to perform gradient ascent and find a local maximum
- max/min has no effect on Newton’s method

#### Second line: polynomial function

- A valid polynomial function is composed of: LHS=RHS
- LHS, expression on the left side of the equation: f(x_1, x_2, x_3…)
  - Ex: for a polynomial with 1 x variable: "f(x_1)"
  - Ex: for a polynomial with 3 x variables: "f(x_1, x_2, x_3)"
- '=': equal sign must be between LHS and RHS
- RHS, expression on the right side of the equation: term ± term ± term ± …
  - Terms are separated by '+' or '-'
  - Ex: "x_1*x_2" has 1 term, and "1 + x_1" has 2 terms
  - Ex: "-x_1" or "-5*x_1" has 1 term since '-' at the beginning is part of the coefficient
  - "x_1*-x_2" is an invalid input; user should simplify the term to "-x_1*x_2"
  - a term is further divided it into a coefficient and x variables
  - term = coefficient \* x_variable \* x_variable \* …
  - Valid characters are: 0 1 2 3 4 5 6 7 8 9 . + - * ^ x _
  - Invalid characters include: ( ) / , and many others
  - Examples of a valid RHS:
    - "-0.5*x_1"
    - "x_1^2 + x_2^2"
    - "1 - 2*x_1 + x_1^2 + 100*x_1^4 - 200*x_1^2*x_2 + 100*x_2^2"
      - this is the simplified polynomial of the Rosenbrock function:
      - 100*(x_2 – x_1^2)^2 + (1-x_1)^2
- User must expand and simplify the polynomial function before running the program
- Examples of a valid polynomial function
  - "f(x_1,x_2) = x_1^2 + x_2^2"
  - "f(x_1,x_2) = -x_1^2 - x_2^2"
  - "f(x_1,x_2) = x_1^2 + x_1*x_2 + x_2^2 + 10"
  - "f(x_1,x_2)=1 - 2*x_1 + x_1^2 + 100*x_1^4 - 200*x_1^2*x_2 + 100*x_2^2"

#### Third line: initial point

- A valid initial point is composed of: initial_point = value of x_1, value of x_2, …
- The third line must start with the string "initial_point="
- The number of x values of an initial point must be equal to the number of x variables of the given polynomial function from the second line
- Examples of valid initial point:
  - For a polynomial with 1 variable, x_1: "initial_point=10.9"
  - For a polynomial with 3 variable, x_1, x_2, x_3: "initial_point=-1, 0, 1"

#### Fourth line: tolerance

- A valid tolerance is composed of: tolerance = tolerance value
- The fourth line must start with the string "tolerance="
- Tolerance level cannot be zero or negative
- Examples of valid tolerance:
  - "tolerance=0.001"
  - "tolerance=0.00001"

#### Fifth line: max_iter

- The maximum number of iterations allowed for the gradient descent algorithm
- A valid max_iter is composed of: max_iter= max number of iterations
- The fifth line must start with the string "max_iter="
- The maximum number of iterations cannot be zero or negative
- Examples of valid tolerance:
  - "max_iter=1000"
  - "max_iter=100000"

**A sample `input_function.txt` file is provided in the current repository:**

- min
- f(x_1,x_2)=1 - 2*x_1 + x_1^2 + 100*x_1^4 - 200*x_1^2*x_2 + 100*x_2^2
- initial_point = 0, 0
- tolerance = 0.000001
- max_iter = 100000

**The optimization results of the sample `input_function.txt` file are also provided in the current repository, please check the files:**

- `output_iterations_gradient_descent.txt`
- `output_iterations_newtons_method.txt`
- `output_results_gradient_descent.txt`
- `output_results_newtons_method.txt`

## Reference List

“Adjugate Matrix.” Wikipedia, Wikimedia Foundation, 12 Dec. 2020, en.wikipedia.org/wiki/Adjugate_matrix.

“Backtracking Line Search.” Wikipedia, Wikimedia Foundation, 12 Dec. 2020, en.wikipedia.org/wiki/Backtracking_line_search.

“Determinant.” Wikipedia, Wikimedia Foundation, 26 Nov. 2020, en.wikipedia.org/wiki/Determinant.

“Exceptions.” Cplusplus.com, www.cplusplus.com/doc/tutorial/exceptions/.

“Gradient Descent.” Wikipedia, Wikimedia Foundation, 8 Dec. 2020, en.wikipedia.org/wiki/Gradient_descent.

Hauser, Kris. “B553 Lecture 6: Multivariate Newton’s Method and Quasi-Newton Methods.” Newtons Method, 25 Jan. 2012, people.duke.edu/~kh269/teaching/b553/newtons_method.pdf.

“Invertible Matrix.” Wikipedia, Wikimedia Foundation, 12 Dec. 2020, en.wikipedia.org/wiki/Invertible_matrix.

“Minor (Linear Algebra).” Wikipedia, Wikimedia Foundation, 2 Dec. 2020, en.wikipedia.org/wiki/Minor_(linear_algebra).

Nocedal, Jorge, and Stephen J. Wright. Numerical Optimization. Springer, 2006.

“Partial Derivative.” Wikipedia, Wikimedia Foundation, 25 Nov. 2020, en.wikipedia.org/wiki/Partial_derivative.

“Polynomial.” Wikipedia, Wikimedia Foundation, 5 Dec. 2020, en.wikipedia.org/wiki/Polynomial.

Polynomials, www.mathsisfun.com/algebra/polynomials.html.

“Rosenbrock Function.” Wikipedia, Wikimedia Foundation, 11 Dec. 2020, en.wikipedia.org/wiki/Rosenbrock_function.

Schlicker, Steve. “Active Calculus - Multivariable.” Second-Order Partial Derivatives, activecalculus.org/multi/S-10-3-Second-Order-Partial-Derivatives.html.

“Second Derivative.” Wikipedia, Wikimedia Foundation, 27 Oct. 2020, en.wikipedia.org/wiki/Second_derivative.

Shoshany, Barak. “Lecture Notes for CSE 701: Foundations of Modern Scientific Programming.” Sept. 2020, baraksh.com/CSE701/notes.php.
